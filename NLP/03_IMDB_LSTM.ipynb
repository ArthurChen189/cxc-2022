{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd0a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9e17a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"C://Users//alvin//DATA//dsc//cxc-2022//data//imdb_ds.csv\"\n",
    "\n",
    "raw_ds = pd.read_csv(data_path)\n",
    "raw_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac7f39bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds[\"sentiment\"].value_counts()\n",
    "\n",
    "# perfectly balanced as all things should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e488e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  sentiment_num\n",
       "0  One of the other reviewers has mentioned that ...  positive              1\n",
       "1  A wonderful little production. <br /><br />The...  positive              1\n",
       "2  I thought this was a wonderful way to spend ti...  positive              1\n",
       "3  Basically there's a family where a little boy ...  negative              0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive              1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dict = {\n",
    "    \"positive\": 1,\n",
    "    \"negative\": 0\n",
    "}\n",
    "\n",
    "raw_ds[\"sentiment_num\"] = raw_ds[\"sentiment\"].map(sentiment_dict)\n",
    "raw_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab2a7e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wonderful little production br br the filming technique is very unassuming very oldtimebbc fashion and gives a comforting and sometimes discomforting sense of realism to the entire piece br br the actors are extremely well chosen michael sheen not only has got all the polari but he has all the voices down pat too you can truly see the seamless editing guided by the references to williams diary entries not only is it well worth the watching but it is a terrificly written and performed piece a masterful production about one of the great masters of comedy and his life br br the realism really comes home with the little things the fantasy of the guard which rather than use the traditional dream techniques remains solid then disappears it plays on our knowledge and our senses particularly with the scenes concerning orton and halliwell and the sets particularly of their flat with halliwells murals decorating every surface are terribly well done'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def cleanup(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    return sentence\n",
    "\n",
    "cleanup(raw_ds[\"review\"].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29449326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "glove_path = \"C://Users//alvin//DATA//dsc//cxc-2022//NLP//glove.6B.50d.txt\"\n",
    "glove_dict = dict()\n",
    "glove_word2idx = dict()\n",
    "glove_vect = []\n",
    "GLOVE_DIM = 50\n",
    "with open(glove_path,'r', encoding=\"utf8\") as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], np.float32)\n",
    "        glove_dict[word] = vector\n",
    "        glove_word2idx[word] = idx\n",
    "        glove_vect.append(vector)\n",
    "        \n",
    "glove_vect = np.stack(glove_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8df191b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_embedding = np.zeros((1,GLOVE_DIM))   #embedding for '<pad>' token.\n",
    "unk_embedding = np.mean(glove_vect, axis=0, keepdims=True)    #embedding for '<unk>' token.\n",
    "glove_vect = np.vstack((glove_vect, pad_embedding, unk_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b19bbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400002, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb80b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello: 13075 \n",
      "\n",
      "kjnkejnv: 400001\n"
     ]
    }
   ],
   "source": [
    "def vectorize_glove(word):\n",
    "    if word in glove_dict.keys():\n",
    "        return glove_word2idx[word]\n",
    "    else:\n",
    "        return len(glove_vect) - 1 #400000\n",
    "    \n",
    "print(\"hello: {} \\n\\nkjnkejnv: {}\".format(vectorize_glove(\"hello\"), vectorize_glove(\"kjnkejnv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e08e7f44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 37668,    835,     41, 400000, 400000, 400000, 400000, 400000,\n",
       "       400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
       "       400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
       "       400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
       "       400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
       "       400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
       "       400000, 400000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SENT_LEN = 50\n",
    "\n",
    "def vectorize_sentence(sentence):\n",
    "    sent_vec = []\n",
    "    words = list(reversed(nltk.word_tokenize(sentence)))\n",
    "    for i in range(MAX_SENT_LEN):\n",
    "        if i < len(words):\n",
    "            sent_vec.append(vectorize_glove(words[i]))\n",
    "        else:\n",
    "            sent_vec.append(len(glove_vect) - 2)\n",
    "            \n",
    "    return np.stack(sent_vec)\n",
    "\n",
    "vectorize_sentence(cleanup(\"I love maths\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52387619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, isTest=False):\n",
    "        self.raw_ds = pd.read_csv(data_path)\n",
    "        sentiment_dict = {\n",
    "            \"positive\": 1,\n",
    "            \"negative\": 0\n",
    "        }\n",
    "\n",
    "        self.raw_ds[\"sentiment_num\"] = self.raw_ds[\"sentiment\"].map(sentiment_dict)\n",
    "        \n",
    "        train_ds, test_ds = train_test_split(self.raw_ds, test_size=0.2, random_state=42)\n",
    "        if isTest:\n",
    "            self.ds = test_ds\n",
    "        else:\n",
    "            self.ds = train_ds\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        x = self.ds[\"review\"].iloc[idx]\n",
    "        y = self.ds[\"sentiment_num\"].iloc[idx]\n",
    "        return vectorize_sentence(cleanup(x)), y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "train_sent = SentimentDataset()\n",
    "test_sent = SentimentDataset(False)\n",
    "\n",
    "train_iter = DataLoader(train_sent, batch_size=40, shuffle=True)\n",
    "test_iter = DataLoader(test_sent, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f54f3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTM(torch.nn.Module):  \n",
    "    def __init__(self, \n",
    "                 in_dim, \n",
    "                 feature_dim = 50,\n",
    "                 hidden_dim = 256,\n",
    "                 out_dim = 2):\n",
    "        super(BiLSTM,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(400000 + 3, 50).from_pretrained(torch.tensor(glove_vect, dtype=torch.float64))\n",
    "        self.lstm = torch.nn.LSTM(feature_dim, hidden_dim, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(0.3) \n",
    "        self.dense = torch.nn.Linear(hidden_dim * 2, out_dim)\n",
    "        \n",
    "    def forward(self, x):   \n",
    "        h0, c0 = self.init_hidden(x)\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        x = torch.squeeze(torch.unsqueeze(x, 0))\n",
    "        out, (hidden,_) = self.lstm(x, (h0,c0))\n",
    "      \n",
    "        x = self.dropout(out[:, -1, :])\n",
    "        return self.dense(x)\n",
    "    \n",
    "    def init_hidden(self, x):\n",
    "        h0 = torch.zeros(4, x.size(0), self.hidden_dim, dtype=torch.double).to(device)\n",
    "        c0 = torch.zeros(4, x.size(0), self.hidden_dim, dtype=torch.double).to(device)\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36467bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 60\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5140bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed_all(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = BiLSTM(MAX_SENT_LEN)\n",
    "model = model.to(device).double()\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=LR)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e703e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---EPOCH 0---\n",
      "Train Acc 0.643325 || Loss 0.6346118450164795\n",
      "Test Acc 0.719275 || Loss 0.5788826942443848\n",
      "---EPOCH 1---\n",
      "Train Acc 0.708525 || Loss 0.5626590847969055\n",
      "Test Acc 0.756775 || Loss 0.506261944770813\n",
      "---EPOCH 2---\n",
      "Train Acc 0.74775 || Loss 0.5122854113578796\n",
      "Test Acc 0.76875 || Loss 0.47928133606910706\n",
      "---EPOCH 3---\n",
      "Train Acc 0.7638 || Loss 0.4799235165119171\n",
      "Test Acc 0.804825 || Loss 0.41965118050575256\n",
      "---EPOCH 4---\n",
      "Train Acc 0.7773 || Loss 0.4576294720172882\n",
      "Test Acc 0.79955 || Loss 0.417822927236557\n",
      "---EPOCH 5---\n",
      "Train Acc 0.78795 || Loss 0.4407944083213806\n",
      "Test Acc 0.82305 || Loss 0.38441962003707886\n",
      "---EPOCH 6---\n",
      "Train Acc 0.799575 || Loss 0.4206980764865875\n",
      "Test Acc 0.83685 || Loss 0.35852834582328796\n",
      "---EPOCH 7---\n",
      "Train Acc 0.808725 || Loss 0.40360331535339355\n",
      "Test Acc 0.84915 || Loss 0.3431132435798645\n",
      "---EPOCH 8---\n",
      "Train Acc 0.8167 || Loss 0.38956189155578613\n",
      "Test Acc 0.86385 || Loss 0.30795103311538696\n",
      "---EPOCH 9---\n",
      "Train Acc 0.82565 || Loss 0.3704417645931244\n",
      "Test Acc 0.872875 || Loss 0.28981146216392517\n",
      "---EPOCH 10---\n",
      "Train Acc 0.83625 || Loss 0.3564716875553131\n",
      "Test Acc 0.888375 || Loss 0.2648877501487732\n",
      "---EPOCH 11---\n",
      "Train Acc 0.84445 || Loss 0.33928051590919495\n",
      "Test Acc 0.868475 || Loss 0.29644617438316345\n",
      "---EPOCH 12---\n",
      "Train Acc 0.856425 || Loss 0.3207390606403351\n",
      "Test Acc 0.90835 || Loss 0.2223951667547226\n",
      "---EPOCH 13---\n",
      "Train Acc 0.863075 || Loss 0.3077753186225891\n",
      "Test Acc 0.920275 || Loss 0.19493228197097778\n",
      "---EPOCH 14---\n",
      "Train Acc 0.872275 || Loss 0.2877822518348694\n",
      "Test Acc 0.9315 || Loss 0.17250443994998932\n",
      "---EPOCH 15---\n",
      "Train Acc 0.89235 || Loss 0.24457381665706635\n",
      "Test Acc 0.9432 || Loss 0.14535613358020782\n",
      "---EPOCH 16---\n",
      "Train Acc 0.896075 || Loss 0.23823267221450806\n",
      "Test Acc 0.9482 || Loss 0.13561773300170898\n",
      "---EPOCH 17---\n",
      "Train Acc 0.900075 || Loss 0.22852292656898499\n",
      "Test Acc 0.9523 || Loss 0.1284491866827011\n",
      "---EPOCH 18---\n",
      "Train Acc 0.90295 || Loss 0.22397595643997192\n",
      "Test Acc 0.95515 || Loss 0.12240104377269745\n",
      "---EPOCH 19---\n",
      "Train Acc 0.90585 || Loss 0.21867111325263977\n",
      "Test Acc 0.958025 || Loss 0.11690541356801987\n",
      "---EPOCH 20---\n",
      "Train Acc 0.908075 || Loss 0.21513064205646515\n",
      "Test Acc 0.960275 || Loss 0.11109127849340439\n",
      "---EPOCH 21---\n",
      "Train Acc 0.90875 || Loss 0.21361181139945984\n",
      "Test Acc 0.95875 || Loss 0.11338184028863907\n",
      "---EPOCH 22---\n",
      "Train Acc 0.90985 || Loss 0.21145357191562653\n",
      "Test Acc 0.96295 || Loss 0.1050315648317337\n",
      "---EPOCH 23---\n",
      "Train Acc 0.912725 || Loss 0.2036229521036148\n",
      "Test Acc 0.965875 || Loss 0.09702558815479279\n",
      "---EPOCH 24---\n",
      "Train Acc 0.911375 || Loss 0.20316387712955475\n",
      "Test Acc 0.967925 || Loss 0.0959504246711731\n",
      "---EPOCH 25---\n",
      "Train Acc 0.9173 || Loss 0.19570812582969666\n",
      "Test Acc 0.96915 || Loss 0.09148094058036804\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m train_iter:\n\u001b[0;32m     14\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 15\u001b[0m     og_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(og_out[:, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, y\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mBiLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x))\n\u001b[0;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(torch\u001b[38;5;241m.\u001b[39munsqueeze(x, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m---> 21\u001b[0m out, (hidden,_) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(x)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:761\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 761\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    762\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    764\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    765\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "test_acc = []\n",
    "test_loss = []\n",
    "\n",
    "\n",
    "for ep_idx in range(EPOCHS):\n",
    "    model.train()\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    for x, y in train_iter:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        og_out = model(x)\n",
    "        outputs = torch.squeeze(og_out[:, 0])\n",
    "        loss = loss_fn(outputs, y.float())\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "        preds = torch.nn.functional.softmax(og_out, dim=1)[:, 0].cpu().detach().numpy() > 0.5\n",
    "        y_np = y.cpu().detach().numpy()\n",
    "\n",
    "        corrects += np.sum(preds==y_np)\n",
    "        total += len(x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_acc.append(corrects/total)\n",
    "    train_loss.append(np.mean(losses))\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    model.eval()\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_iter:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            og_out = model(x)\n",
    "            outputs = torch.squeeze(og_out[:, 0])\n",
    "            loss = loss_fn(outputs, y.float())\n",
    "            losses.append(loss.cpu().detach().numpy())\n",
    "            preds = torch.nn.functional.softmax(og_out, dim=1)[:, 0].cpu().detach().numpy() > 0.5\n",
    "            y_np = y.cpu().detach().numpy()\n",
    "\n",
    "            corrects += np.sum(preds==y_np)\n",
    "            total += len(x)\n",
    "            \n",
    "    test_acc.append(corrects/total)\n",
    "    test_loss.append(np.mean(losses))\n",
    "    \n",
    "    print(\"---EPOCH {}---\".format(ep_idx))\n",
    "    print(\"Train Acc {} || Loss {}\".format(train_acc[-1], train_loss[-1]))\n",
    "    print(\"Test Acc {} || Loss {}\".format(test_acc[-1], test_loss[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
